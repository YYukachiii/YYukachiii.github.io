[{"title":"用PyTorch学习NLP Chapter 1.神经网络基础组件","date":"2019-09-10T06:31:09.000Z","path":"2019/09/10/NLP/Chap.3/","text":"Chapter 3.神经网络基础组件Perceptron: The Simplest Neural Network​ 图解感知机结构：(图的链接是在是太长了，要考虑一下解决办法) ​ 激活函数的选择有很多种，这里选择了Sigmoid函数。 Activation Functions​ 这里列出本书中用到的四个激活函数：(都已经比较熟悉了，就不多做解释了) Sigmod 12x = torch.range(-5, 5, 0.1)y = torch.sigmoid(x) Tanh 12x = torch.range(-5, 5, 0.1)y = torch.tanh(x) ReLU 12x = torch.range(-5, 5, 0.1)y = torch.relu(x) Softmax 123softmax = torch.nn.Softmax(dim=1)x = torch.randn(1,3)y = softmax(x) Loss Functions Mean Squared Error Loss Categorical Cross-Entropy Loss PyTorch中的计算交叉熵的对象的输入是未经过Softmax变换的预测值和各个样本的类标号构成的向量(为转换成One-hot向量) Binary Cross-Entropy 不同于Categorical Cross-Entropy Loss，二元交叉熵的输入是预测值的概率分布和样本的类标号构成的向量 逻辑回归的损失函数实际上就是二元交叉熵损失 Diving Deep into Supervised Training​ 统计学习方法的三要素： 模型 策略(经验风险最小化、结构风险最小化) 算法(BP等等) ​ 完成一个有监督学习任务还需要训练数据。 Putting It Together:Gradient-Based Supervised Learning​ PyTorch完成一个学习过程的步骤如下： 选择模型和用于训练模型的数据 使用名为zero_grad()的函数清除存储在模型对象中的所有信息(如梯度) 模型计算给定输入数据(x_data)的输出(y_pred) 计算损失函数，通过比较模型输出(y_pred)和y_target来计算损失 PyTorch损失对象(criteria)具有一个名为backward()的函数，该函数迭代地通过计算图向后传播损失，并将梯度通知给每个参数 最后，优化器(optim)用一个名为step()的函数指示参数如何在已知梯度的情况下更新参数 至此，完成了一次参数的更新。 Correctly Measuring Model Performance:Splitting the Dataset​ 我们训练模型的最终目标是使模型能够很好的概括数据的真实分布。然而数据有限，所以其实数据的真实分布是不得而知的。因此当模型过度拟合训练数据的分布时，因为训练数据不能够代表全部数据的真实分布，这个时候就会产生过拟合。 ​ 如何避免过拟合(顺便提一嘴)： 增加训练数据 降维 正则化 ​ 如何选择模型： 在训练数据比较充足时，可以将训练数据划分为训练集、验证集和测试集 大多数情况下，训练数据没有那么充裕，这个时候可以使用交叉验证 Knowing When to Stop Training 选择模型训练的epoch数目一般使用启发式发放，一般称为早停止 在每个epoch或每N个epoch结束后，查看模型在验证集上的结果，如果验证集准确率要高于之前的模型，则保留当前训练的模型副本，否则停止训练 将测试性能最优的模型作为最后的训练结果 Regularization L2正则化 在pytorch中，可以通过在优化器(optim)中设置weight_decay参数来控制正则化的程度 结构正则化技术 Dropout L1正则化 鼓励稀疏解 Example：Classifying Sentiment of Restaurant ReviewsThe Yelp Review Dataset​ Yelp评论数据集最初是2015年由Yelp(美国最大的点评网站)举办的文本情感分类比赛中提出的，其中有56万个训练样本和3.8万个测试样本。在这里是选择了10%的训练数据作为完整数据集。 ​ 使用一个小数据集可以快速进行试验，可以使用从较小数据集子集中获得的知识对整个数据集进行重新训练。在训练深度学习模型时，这是一个非常有用的技巧。 通过训练集来得到模型的参数 通过验证集选择最优的超参数 通过测试集对模型进行最终的评估和报告 ​ 在几乎每个实例中， 都使用Vocabulary，Vectorizer和Dataset这三个类来执行一个关键的pipeline：将文本输入转化为小批量的向量。 Understanding PyTorch’s Dataset Representation​ PyTorch通过提供 class Dataset为数据集提供了一个抽象，在对新数据集使用PyTorch时，新的数据集类必须继承Dataset类，并实现getitem和len方法。 ​ 通过针对每个实际的数据集实现继承PyTorch的新的数据集类，允许各种PyTorch程序使用新实现的数据集类。 ​ ReviewDataset Class的结构： __init__() 重写构造函数，为各种变量赋值 self.review_df - 数据集(什么样的数据集？) self._vectorizer - 向量化的数据集 训练集、验证集、测试集变量 其他 @classmethod load_dataset_and_make_vectorizer() 加载数据集并创建向量化的实例 @classmethod 是类的内置函数，它能够自己创建一个类的实例 @classmethod load_dataset_and_load_vectorizer() 加载数据集和向量化的实例 @classmethod 这两个内置函数实际上完成了两个任务 在没有正确格式的数据集和向量时先对数据进行格式化 然后再创建Dataset的实例 @staticmethod load_vectorizer_only() 仅加载向量 save_vectorizer() 将向量化的实例使用json存储到硬盘 get_vectorizer() 返回向量(仅仅是将类的属性返回) get_split() 选择将要处理的数据，并给类的对应属性赋值 __len__(self) 返回要处理数据的size __getitem__() 重写的PyTorch的Dataset类的方法 暂时没看明白是干什么的》。。。 get_num_batcher() 输入批的大小，返回批的数量 The Vocabulary, the Vectorizer, and the DataLoaderVocabulary​ 从文本到向量化的minibatch处理的第一步是将每个token映射到它的向量标号。实现方法是在token和标号之间有一个双向映射。这两个映射被封装到Vocabulary中。 ​ Vocabulary类的结构如下： __init()__ self._token_to_idx 一个完成从token到ID的映射的字典 self._idx_to_token 完成从id到token的映射 to_serializable() 返回一个可以被序列化的字典 @classmethod from_serializable() 从一个序列化的字典(to_serializable返回的对象)中创建Vocabulary实例 add_token() 根据token更改两个映射字典 add_many() 添加许多个token到Vocabulary中 lookup_token() 查找token的index，如果token没出现，就返回UNK的index lookup_index() 查找index对应的token __str__() 返回self的大小(?) __len__() 返回映射的大小 Vectorizer​ ReviewVectorizer()的结构： __init__() self.review_vocab 将词(word)与整数匹配 self.rating_vocab 将类别标签与整数匹配 vectorize() 创建一个review的one-hot向量 @classmethod form_dataframe() 从Dataset 的dataframe结构实例化向量(?究竟干了啥) 创建了一个一样的向量 @classmethod from_serializable() 从serializable dictionary 创建一个评论向量 to_serializable() Create the serializable dictionary for caching Dataloader​ 文本向量化小批处理的最后一个阶段是对向量化的数据分组。PyTorch提供了一个名为DataLoader的内置类来协调这个过程。DataLoader通过提供一个PyTorch数据集、一个batch_size和一些关键字参数来实例化，得到的对象是一个python迭代器。 ​ 本例中，通过在定义函数generate_batcher()中使用DataLoader内置类，来实现batch的迭代。 12def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"): ...... A Percptron Classifier​ 定义一个ReviewClassifier类，来实现计算模型的预测值。 12class ReviewClassifier(nn.Module): ...... The Training Routine​ 这里的训练模型的历程可以看做一种规范，在深度学习的开发中，可以将这看做一种习惯。 ​ 使用python内置的argparse模块来进行参数的管理： 123from argparse import Namespaceargs = Namespace(......) Setting The Stage For The Training To Begin 初始训练状态 实例化数据集和模型 实例化损失函数和优化器 123456789101112131415161718import torch.optim as optimdef make_train_state(args): ...... train_state = make_train_state(args)# dataset and vectorizerdataset = ReviewDataset.load_dataset_and_make_vectorizer(arts.review_csv)vectorizer = data.get_vectorizer()# model,仅仅给出预测classifier = ReviewClassifier(num_features = len(vectorizer.review_vocab))classifier = classifier.to(args.device)# loss and optimizer， 计算损失并进行优化loss_func = nn.BCEWithLogitsLoss()optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate) The Training Loop​ 训练循环由两个循环组成：内循环覆盖数据集中的mini-batch，另一个外循环重复内循环若干次。在每个内部循环中，计算损失，并使用优化器更新模型参数。 1234567891011121314151617for epoch_index in range(args.num_epoch): # 初始化参数 for batch_index, batch_dict in enumerate(batch_generator): # 1. zero the gradients # 2. compute the output # 3. compute the loss # 4. use loss to produce gradients # 5. use optimizer to take gradient step # compute the accuracy # 在训练完每个epoch之后在计算模型在验证集上的准确率，从而判断是否发生了过拟合 for batch_index, batch_dict in enumerate(batch_generator): # 1. compute the output # 2. compute the loss # 3. compute the accuracy # 结束一个epoch的训练过程，中间过程的结果都被保存到了train_state中 Evaluation, Inference, and InspectionEvaluation, Inference, and Inspection​ 在测试集上进行验证的过程与在验证集上进行计算的过程完全相同。 Inference And Classifying New Data Points​ 在新数据上进行推断，看模型效果如何。 Inspecting Model Weights​ 另一种检查模型在完成训练后是否表现良好的方法是检查权重，对权重是否正确做出判断。 ​ 对当前的这个任务来说这种方法是可行的，但是并不通用。 Summary","tags":[{"name":"NLP","slug":"NLP","permalink":"http://yyukachiiii.github.io/child/tags/NLP/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://yyukachiiii.github.io/child/tags/PyTorch/"}]},{"title":"对话系统","date":"2019-09-09T04:24:20.000Z","path":"2019/09/09/论文阅读/人机对话/","text":"人机对话系统综述，2018，哈工大人机对话系统的研究背景及意义​ 人机对话系统的研究最早追溯到1950年，即图灵测试。测试者借助某种装置已对话的方式与人类或对话系统进行交谈，当测试结束后，如果有30%以上的测试者不能正确区分对话系统和人的回复，则称该系统通过了图灵测试，拥有了人的智能。 ​ 人机对话系统的功能分类： 任务型 目的：完成特定任务 应用场景：虚拟个人助理 开放域聊天 目的：闲聊 应用场景：娱乐、情感陪护、营销沟通 知识问答 目的：知识获取 应用场景：客服、教育 推荐 目的：信息推荐 应用场景：个性化推荐 发展历史 最早的人机对话系统：诞生于1966年，由MIT的Joseph Weizenbaum开发的ELIZA 作用：用于在临床治疗中模仿心理医生 1988年，UC Berkeley开发了名为UC(UNIX Consultant)的对话系统 作用：帮助用户学习怎样使用UNIX操作系统 地位：推动了对话系统的智能化程度 Richard S. Wallace在1995年开发了ALICE系统，并与1998年开源 模型和框架​ 一般的人机对话系统通常包括三个关键模块： 口语语言理解(SLU):将用户说出的话转换为结构化的语义表示。 举例：识别出“领域、意图和语义槽(slot)” 对话管理(DM) 定义：综合用户当前输入和历史对话中已获得的信息，给出机器答复的结构化表示。 结构 对话状态追踪(Dialogue State Tracking, DST) 作用：依据SLU的结果，把旧的对话状态更新为新的对话状态 对话策略优化(Dialogue Policy Optimization, DPO) 作用：根据DST维护的对话状态，确定当前状态下如何回复 自然语言生成(NLG) 定义：把DM输出的结构化对话策略还原成对人友好的自然语言 实现技术和方案​ 以上框架是针对任务型对话系统提出的，并不能够很好的应对开放域聊天型对话系统。因为在开放领域中，无法穷举用户的意图和语义槽，也无法穷举可能的回复策略。 ​ 但随着深度学习技术的发展，用户的意图和回复策略等信息可以使用向量隐式的表示。而且可以自动的从大量的对话数据中学习出来。 ​ 任务型和开放域聊天机器人不同模块的对比： 任务型机器人 对话语言理解 领域及意图识别、语义槽识别 对话管理 状态跟踪、对话策略 自然语言生成 基于模板(穷举) 开放域聊天 对话语言理解 主题识别、关键词识别、情感分析 对话管理 记忆网络、对话上下文建模 自然语言生成 基于深度学习的编码-解码 ​ 当前研究热点： 使用深度学习技术实现任务型对话的各个模块 使用端到端技术直接生成自然语言回复 结合外部知识库进行自然语言生成 服务平台​ 开发一个对话系统的步骤： 定义和准备 定义该对话系统支持的领域/意图和问题类型 定义语义槽 进行相关的数据标注 训练和实现 训练领域/意图或问题分类模型 实现语言理解、对话管理和语言生成模块 部署 在自己的服务器上，部署实现好的工程，对用户提供服务 总结及展望​ 当前的人机对话系统的主要问题： 在聊天上如何让机器更像人 在场景化任务中如何做到高效的场景切换 基于多轮交互的人机对话系统综述，2019，中科大引言 单轮对话一般不存在指代省略或上下文连贯性问题 多轮对话中存在人机的多轮交互，对聊天的上下文、指代省略的补全和复杂需求的明确的等问题都有更为复杂的要求 早期的对话系统的研究，基本都是采用基于符号和规则的方法，至今仍然是主流的人机对话系统的解决方案之一 基于多轮交互的人机对话系统主要类型及解决方案任务型对话基于管道的方法​ 任务型对话的一种经典的解决方案，主要结构包括： 自然语言理解 对话管理 对话状态追踪 对话策略学习 自然语言生成 自然语言理解 输入：用户的语句 任务 槽标记(slot tagging) 对句子中的语义槽序列进行标记 语义槽是根据不同的场景预先定义的 大量基于深度神经网络的方法被应用到这个问题上 领域识别(domain detection) 将用户的服务需求划分到某一事先定义好的领域下 意图理解(intent determination) 针对用户的表述中的显示意图和隐示意图做出识别和理解 地位 对话系统中的预处理模块","tags":[{"name":"NLP","slug":"NLP","permalink":"http://yyukachiiii.github.io/child/tags/NLP/"},{"name":"对话系统","slug":"对话系统","permalink":"http://yyukachiiii.github.io/child/tags/对话系统/"}]},{"title":"文本摘要论文阅读","date":"2019-09-08T11:43:45.000Z","path":"2019/09/08/论文阅读/文本摘要/","text":"文本摘要常用数据集和方法研究综述，2019，中科院，中科院大学引言 任务目的 从一篇或多篇主题相同的文本中抽取能够反映主题的精简压缩版本，帮助用户快速形成对特定主题文本内容的全面了解 任务分类 输入文本数量 单文本摘要方法 多文本摘要方法 抽取方式 抽取式摘要 从原文中不加修改的抽取文本片段组成摘要 生成式摘要 重新组织句子形成比抽取式摘要更加精简的形式 摘要目标 面向查询的摘要(阅读理解？问答？) 一般总结性摘要 领域需求 医学摘要 邮件摘要 …… 文章内容总结 常用数据集总结 做针对数据集的方法总结和分析 总结常用数据集和方法的研究现状、存在的问题 文本摘要常用数据集总体概况​ 文本摘要常用数据集大致可分为两类： 公用数据集 自建数据集 ​ 基于深度神经网络的文本摘要需要较大规模(十万级规模)的训练数据。 DUC/TAC 描述 人工标注的生成式摘要数据集 规模较小(百篇规模),不适用于训练深度神经网络模型 主要方法 基于图模型的方法 基于传统机器学习的方法 Gigaword 描述 由英文新闻文章组成的数据集，包括950万多个新闻源的新闻语料 生成方式 将文章的首句话与新闻提要组成生成式摘要语料库 特点 原句与摘要句都是单个句子 CNN/Daily Mail 描述 单文本摘要语料库 每篇摘要包含多个摘要句 LCSTS(Large scale Chinese Short Text Summatization dataset) 描述 短文本新闻摘要数据库 来源 新浪微博 规模 超过200万 特点 文本篇幅较短 存在噪声 以提出的方法 利用RNN提取生成式摘要 注意力机制 NLPCC(自然语言处理与中文计算会议) 背景 NLPCC是由CCF举办的自然语言文本评测会议，包括文本摘要、情感分析、自动问答等任务 在NLPCC出现的文本摘要任务均为单文本摘要 特点 新闻文本不分领域、不分类型 篇幅较长 已有方法 。。。 自建数据集及其对应方法​ 公用数据集较少，因此基于自建数据集的摘要任务，常用方法可分为： 基于统计的方法 基于图模型的方法 基于词法链的方法 基于篇章结构的方法 基于机器学习的方法 经典算法和最新方法用到的数据集​ 经典方法和最新方法大都是基于深度学习的方法，但是也包括LexRank、TextRank等经典方法。 结论 缺少大规模中文长文本数据集","tags":[{"name":"NLP","slug":"NLP","permalink":"http://yyukachiiii.github.io/child/tags/NLP/"},{"name":"文本摘要","slug":"文本摘要","permalink":"http://yyukachiiii.github.io/child/tags/文本摘要/"},{"name":"论文阅读","slug":"论文阅读","permalink":"http://yyukachiiii.github.io/child/tags/论文阅读/"}]},{"title":"阅读理解论文阅读","date":"2019-09-08T08:19:55.000Z","path":"2019/09/08/论文阅读/阅读理解/","text":"基于深度学习的机器阅读理解综述，2019，北航引言 定义 让机器学会阅读和理解文章，对于给定的问题，从相关文章中寻找答案 设计技术 语言理解 知识推理 摘要生成 应用 智能搜索：搜索互联网的文档进行阅读理解，为用户返回更加准确和智能的答案 智能客服 发展历史 1999年开始了最早的MRC研究 传统的MRC技术大多采用模式匹配的方法进行特征提取，不能处理表达的多样性问题 先进的MRC技术采用深度神经网络进行机器阅读理解的研究 2016年斯坦福发布大规模数据集SQuAD，极大推动了MRC的快速发展 总结： 历史比较短，研究内容都比较新 基于深度学习的阅读理解 模型的组成 词向量模块 将所有单词映射到一个向量空间，包含单词的语法和语义信息，及词与词之间的关系 编码模块 以词向量表示的文本序列作为输入，通过深度神经网络对文本序列进行特征提取，含有上行下文信息和语义信息 注意力模块 从文章中挑选出与问题关联度最大的部分内容，排除不相关信息 答案预测模块 方法由抽取式逐渐发展为生成式；由单篇文章提供转向多篇文章生成 词向量模块​ 词向量的生成方式： 矩阵分解法 参数学习法 上下文学习法 参数学习法​ 常见的参数学习法： Bengio等基于神经网络的概率语言模型，将词向量作为语言模型的参数进行学习 Mikolov提出的Word2vec模型，借鉴N-gram思想，没有考虑全局信息 ​ 缺陷： 没有解决一词多义的问题 上下文学习法 参数学习法是词向量的一种静态的表示方法，同一个词汇具有相同的词向量表示 解决一词多义问题关键在于利用上下文语境，词向量是动态的模型输出 ​ 上下文学习法学习到的词向量举例： BiLSTM ELMo(Embeddings from Language Models) 注意力机制​ 处理阅读理解问题时，基于神经网络的模型几乎都使用了注意力机制。 ​ 在MRC中，根据注意力机制的结构，可分为： 单路注意力模型 双路注意力模型 自匹配注意力模型 单路注意力模型 描述： 模拟人做阅读理解的过程 只在文章上使用注意力机制 通过结合问题和文本段落二者的信息，生成一个关于文本段落各部分的注意力权重，对文本信息进行加权 双路注意力模型 描述： 同时在问题和文章上使用注意力机制 对文章和问题之间进行了细粒度的建模，一般比单路注意力模型好 自匹配注意力模型 描述 反复阅读文章并找出重点 注意力模块是机器阅读理解的核心模块，对提升模型的性能至关重要 答案预测​ 主要分类： 抽取式 生成式 答案抽取 定义 从文章中挑取答案，然后生成答案 举例 斯坦福大学的SQuAD数据集中，每个问题的答案都是原文的一个子片段，是典型的抽取式数据集 抽取模型 序列模型 边界模型 答案生成 举例 微软亚洲研究院的MS MARCO数据集，答案是人工阅读候选文章后总结生成的，不再受限于文章片段，要求模型具有生成答案的能力 实例 通过答案抽取模型从候选文章中提取线索，然后用机器翻译模型将线索翻译为答案 现状 抽取模型仍是主流，生成技术只是作为一种辅助手段产生答案 MRC面临的主要问题词向量模块仍需改善 使用预训练的语言模型取代预训练的词向量对MRC模型带来了显著的提升 但基于语言模型的词向量仍然有局限性 模型缺乏推理能力 当前的阅读理解模型仍不具备推理能力，而是注意某些线索以执行粗浅的模式匹配 模型缺乏外部知识 模型的信息均来自于文章，没有结合外部知识 答案生成技术研究不足 当前的答案生成技术仍然以抽取式为主，因此在SQuAD数据集上取得了成功 但是在MS MACRO等贴近真实应用场景的数据集仍效果欠佳 MRC的发展趋势 构建端到端的高效模型 深层结构探索，提高推理能力 与其他NLP技术进行结合 外部知识库 指代消解 答案生成技术的深入研究 对多候选文章进行排名 问题 什么是预训练语言模型？ 最新的SQuAD数据集由500+文章和10,0000+问题组成，训练集20MB左右，发展集4MB左右，仅训练数据来看并不大","tags":[{"name":"NLP","slug":"NLP","permalink":"http://yyukachiiii.github.io/child/tags/NLP/"},{"name":"论文阅读","slug":"论文阅读","permalink":"http://yyukachiiii.github.io/child/tags/论文阅读/"},{"name":"阅读理解","slug":"阅读理解","permalink":"http://yyukachiiii.github.io/child/tags/阅读理解/"}]},{"title":"自动问答论文阅读","date":"2019-09-08T07:07:34.000Z","path":"2019/09/08/论文阅读/自动问答/","text":"自动问答综述，2002，哈尔滨工业大学引言 传统的搜索引擎的不足 相关性信息太多 检索需求无法以几个关键字的逻辑组合来完全表达 以关键字为基础的索引停留的语言层面，没有涉及到语义 自动问答系统 为了改进搜索引擎的弊端发展而来 问答系统就是新一代的搜索引擎 研究概况 早起(上世纪80年代)的问答系统一直被限制在特殊领域的专家系统 组成(三个部分)： 问题分析：理解用户的问题是什么 问题分类 关键词提取 关键词扩展 信息检索：在已有的文档中查找和关键词集相关的文档 答案抽取：从信息检索中返回的网页中抽取答案 最为影响整个问答系统准确性的部分 常问问题(FAQ)库：常问问题可以直接从库中检索并返回 问题分析​ 需要完成的任务： 确定问题的类型 提取出问题的关键词 依据问题的类型对关键词进行适当的扩展 问题分类​ 针对不同类型的问题，有不同的处理方法，例如询问人、询问时间、询问数量等等。 ​ 对问题分类之后，在针对不同的问题类型，制定不同的答案抽取规则。 ​ 问题分类有两种方法： 按照事先规定好的类别进行分类 使用机器学习算法自动分类 关键词提取​ 在问题中，提取出对检索有用的关键字。关键词能够提高检索系统的准确性。 关键词扩展​ 答案中的词常常不是问题的关键词，而是关键词的同义扩展。 ​ 关键词扩展虽然能够提高系统的召回率，但是扩展不当却会降低检索的正确率。 信息检索模块 任务 用前面提取出的关键字到文档库中查找相关的文档 信息检索模块的建立 对文档库进行预处理(汉语要分词、英语要Stemming) 对文档库建立索引 返回内容 文档、段落或句子 答案抽取 任务 将信息检索模块搜索出来的相关文档抽取答案 以句子作为答案​ 步骤如下：（类似于文本摘要呀） 把检索出来的文档分成句子 按照一定的算法，计算每个句子的权重 对句子按照权重进行排序 根据问题的类型对候选答案重新排序 以词或短语作为答案以文摘作为答案​ 多文档自动摘要技术，把相关文档做成文摘，在把文摘返回给用户。 评价​ 建立测试集，将系统对测试集问题得到的回答与人工答案进行对比，计算出问答系统的准确率。 结论 当前(2002年)的问答系统不具备思维和推论能力，只是从文档库中搜索相关的答案。自动问答技术处于起步阶段。","tags":[{"name":"NLP","slug":"NLP","permalink":"http://yyukachiiii.github.io/child/tags/NLP/"},{"name":"论文阅读","slug":"论文阅读","permalink":"http://yyukachiiii.github.io/child/tags/论文阅读/"},{"name":"问答系统","slug":"问答系统","permalink":"http://yyukachiiii.github.io/child/tags/问答系统/"}]},{"title":"用PyTorch学习NLP Chapter 1.基础介绍","date":"2019-09-06T13:20:36.000Z","path":"2019/09/06/NLP/Chap.1/","text":"Chapter 1.基础介绍The Supervised Learning Paradigm（范例）符号表示 输入(观察值)：$x$ 类别标签(ground truth)：$y$ 类别的预测：$\\hat{y}$ 有价值的内容 实际应用中很少使用随机梯度下降(SGD),因为收敛非常慢 Observation And Target Encoding文本的向量表示的方法 One-Hot Representation 表示成句子或文档长度乘以词表大小的矩阵 表示成一个词汇表长度的向量(一般都是使用这种表示吧…) TF Representation TF表示是构成句子的词的one-hot的总和，即向量中每个条目是相应单词在句子中出现次数的计数 TF-IDF(Inverse Document Frequency) Representation 在TF表示的基础上，惩罚出现次数更多地条目，奖励罕见的符号 这样的启发式表示(?什么是启发式表示？？？)在深度学习中很少使用 Target Encoding 按照目标任务的具体情况选择文本的向量表示 Word Embedding 词向量的分布式表示 Computional Graphs 在计算图中，结点是乘法和加法等数学运算，输入是节点的传入边，输出是结点的传出边。 PyTorch Basics动态计算图 分类 TensorFlow Caffe(贾扬清开发的深度学习框架) Theano 特点 在计算之前，需要声明、编译和执行计算图 计算效率高，在生产中非常有用 静态计算图 分类 PyTorch Chainer DyNet 特点 更加灵活，不需要在每次执行(计算)之前再进行编译 每个输入(什么意思?)可能导致不同的图结构 张量 定义 零阶张量：标量(数字) 一阶张量：向量（数字数组） 二阶张量：矩阵（向量数组）","tags":[{"name":"NLP","slug":"NLP","permalink":"http://yyukachiiii.github.io/child/tags/NLP/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://yyukachiiii.github.io/child/tags/PyTorch/"}]},{"title":"nexmoe","date":"2019-08-29T01:16:49.000Z","path":"2019/08/29/nexmoe/","text":"​ 从零开始学习创建基于hexo框架的主题模型。 前置知识模板引擎定义 模板引擎是为了使用户界面与业务数据（内容）分离而产生的，它可以生成特定格式的文档，用于网站的模板引擎就会生成一个标准的文档. ​ 简单来说，就是将模板文件和数据通过模板引擎生成一个HTML代码。能够让动态页面在渲染的时候，能够简化字符串的拼接操作。 分类 ejs EJS 是一套简单的模板语言，帮你利用普通的 JavaScript 代码生成 HTML 页面。 CSS定义 层叠样式表(Cascading Style Sheets)是一种用来表现HTML（标准通用标记语言的一个应用）或XML（标准通用标记语言的一个子集）等文件样式的计算机语言。 CSS预处理器定义 CSS预处理器是用一种专门的编程语言，进行Web页面样式设计，然后再编译成正常的CSS文件，以供项目使用。 与CSS的关系​ 使用CSS语言之外的语言进行网页样式设计的代码，经过预处理器处理后，转换为标准CSS文件。 代码答疑HEXO辅助函数模板partial 作用：载入其他模板文件，在该代码的位置插入模板文件的代码 代码：&lt;%- partial(layout, [locals], [options]) %&gt; 详细信息 ejs相关代码 &lt;% %&gt;的作用？ 在写网页代码时，有时需要用JavaScript的逻辑代码来渲染页面，但是JavaScript的代码与HTML的代码是不一样的，不好区分。 因此使用&lt;% %&gt;来包裹住逻辑代码，方便与HTML代码进行区分。","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yyukachiiii.github.io/child/tags/hexo/"}]}]